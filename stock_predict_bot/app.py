"""
–¢–µ–ª–µ–≥—Ä–∞–º-–±–æ—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –∞–∫—Ü–∏–π.
–§—É–Ω–∫—Ü–∏–∏:
- –∑–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫ (yfinance, 2 –≥–æ–¥–∞, –∫–æ–ª–æ–Ω–∫–∞ 'Close');
- –æ–±—É—á–µ–Ω–∏–µ 3 –º–æ–¥–µ–ª–µ–π: Ridge(–ª–∞–≥–∏ —Å –ø–æ–¥–±–æ—Ä–æ–º alpha), ARIMA (—Å auto_arima), LSTM (—Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π);
- —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–æ RMSE –∏ MAPE –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º —Ö–≤–æ—Å—Ç–µ;
- –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 30 –¥–Ω–µ–π, –≥—Ä–∞—Ñ–∏–∫ (–∏—Å—Ç–æ—Ä–∏—è + –ø—Ä–æ–≥–Ω–æ–∑);
- —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ª–æ–∫–∞–ª—å–Ω—ã–º –º–∏–Ω–∏–º—É–º–∞–º/–º–∞–∫—Å–∏–º—É–º–∞–º;
- –æ—Ü–µ–Ω–∫–∞ —É—Å–ª–æ–≤–Ω–æ–π –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ –≤–≤–µ–¥—ë–Ω–Ω—É—é —Å—É–º–º—É;
- –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤ logs.csv.

–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: aiogram, yfinance, numpy, pandas, scikit-learn, statsmodels, matplotlib, tensorflow (–¥–ª—è LSTM), pmdarima (–¥–ª—è auto_arima).
–ï—Å–ª–∏ TensorFlow –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, LSTM –ø—Ä–æ–ø—É—Å–∫–∞–µ—Ç—Å—è.
–ï—Å–ª–∏ pmdarima –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, ARIMA –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã.
"""

import os
import io
import math
import logging
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from datetime import datetime, timedelta, timezone

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from aiogram import Bot, Dispatcher, F, types
from aiogram.filters import Command
from aiogram.enums import ParseMode
from aiogram.utils.keyboard import InlineKeyboardBuilder

import yfinance as yf
from sklearn.linear_model import Ridge
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import GridSearchCV

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
from statsmodels.tsa.arima.model import ARIMA

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∞ pmdarima –¥–ª—è auto_arima
PMDARIMA_AVAILABLE = True
try:
    from pmdarima import auto_arima
except Exception:
    PMDARIMA_AVAILABLE = False

TF_AVAILABLE = True
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import layers
    from tensorflow.keras.callbacks import EarlyStopping
except Exception:
    TF_AVAILABLE = False

# –ë–∞–∑–æ–≤–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–µ—Ä–∞ (–¥–ª—è –∫–æ–Ω—Å–æ–ª–∏)
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("stock-bot")

# –°–ø–∏—Å–æ–∫ –ø–æ–ø—É–ª—è—Ä–Ω—ã—Ö —Ç–∏–∫–µ—Ä–æ–≤
TOP_TICKERS = [
    "AAPL", "MSFT", "GOOGL", "TSLA", "AMZN", "NVDA", "META", "NFLX", "BABA", "TSM",
    "V", "JPM", "WMT", "DIS", "KO", "ORCL", "CRM", "AMD", "INTC", "IBM"
]

# –°–æ—Å—Ç–æ—è–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ç–∏–∫–µ—Ä–∞ –∏ —Å—É–º–º—ã
user_states: Dict[int, Dict[str, str]] = {}


# –í–°–ü–û–ú–û–ì–ê–¢–ï–õ–¨–ù–´–ï –®–¢–£–ö–ò

@dataclass
class EvalResult:
    name: str
    rmse: float
    mape: float
    model_obj: object
    extra: Dict[str, object]

def mape(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    """MAPE –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö (–¥–æ–±–∞–≤–ª—è–µ–º —ç–ø—Å–∏–ª–æ–Ω)."""
    eps = 1e-8
    return float(np.mean(np.abs((y_true - y_pred) / (np.abs(y_true) + eps))) * 100.0)

def train_test_split_series(series: pd.Series, test_size_days: int = 30) -> Tuple[pd.Series, pd.Series]:
    """–†–∞–∑–±–∏–≤–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω–æ–π —Ä—è–¥ –Ω–∞ train/test –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º –¥–Ω—è–º (–ø–æ –∏–Ω–¥–µ–∫—Å—É)."""
    if len(series) <= test_size_days + 30:
        # –∑–∞–ø–∞—Å –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        test_size_days = max(7, len(series) // 5)
    return series.iloc[:-test_size_days], series.iloc[-test_size_days:]

def remove_outliers(series: pd.Series, threshold: float = 3.0) -> pd.Series:
    """–£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã —Å –ø–æ–º–æ—â—å—é Z-score."""
    z_scores = np.abs((series - series.mean()) / series.std())
    return series[z_scores < threshold]

def add_technical_indicators(df: pd.DataFrame) -> pd.DataFrame:
    """–î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ—Å—Ç—ã–µ —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã: RSI, MACD."""
    # RSI
    delta = df['y'].diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=14, min_periods=1).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=14, min_periods=1).mean()
    rs = gain / loss
    df['rsi'] = 100 - (100 / (1 + rs))
    
    # MACD
    ema12 = df['y'].ewm(span=12, adjust=False).mean()
    ema26 = df['y'].ewm(span=26, adjust=False).mean()
    df['macd'] = ema12 - ema26
    df['macd_signal'] = df['macd'].ewm(span=9, adjust=False).mean()
    
    return df

def build_lag_features(y: pd.Series, max_lag: int = 10, ma_windows: List[int] = [3, 7, 14]) -> pd.DataFrame:
    """
    –ü—Ä–∏–Ω–∏–º–∞–µ–º Series/ndarray/DataFrame.
    –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º –≤ 1D Series (–µ—Å–ª–∏ DataFrame ‚Äî –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü).
    –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å, –µ—Å–ª–∏ –æ–Ω —Å–æ–≤–º–µ—Å—Ç–∏–º –ø–æ –¥–ª–∏–Ω–µ.
    –î–æ–±–∞–≤–ª—è–µ–º —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã.
    """
    # –ï—Å–ª–∏ —ç—Ç–æ DataFrame ‚Äî –±–µ—Ä—ë–º –ø–µ—Ä–≤—ã–π —á–∏—Å–ª–æ–≤–æ–π —Å—Ç–æ–ª–±–µ—Ü
    if isinstance(y, pd.DataFrame):
        num_cols = y.select_dtypes(include=[np.number])
        if num_cols.shape[1] == 0:
            raise ValueError("–í—Ö–æ–¥–Ω–æ–π DataFrame –Ω–µ —Å–æ–¥–µ—Ä–∂–∏—Ç —á–∏—Å–ª–æ–≤—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤.")
        y_series = num_cols.iloc[:, 0]
    else:
        y_series = y

    # –ò–Ω–¥–µ–∫—Å, –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω
    orig_index = getattr(y_series, "index", None)

    # –ü—Ä–∏–≤–æ–¥–∏–º –∫ np –∏ —Å–ø–ª—é—â–∏–≤–∞–µ–º
    arr = np.asarray(y_series)
    arr = np.squeeze(arr) 
    if arr.ndim != 1:
        raise ValueError(f"–û–∂–∏–¥–∞–ª—Å—è –æ–¥–Ω–æ–º–µ—Ä–Ω—ã–π —Ä—è–¥, –∞ –ø—Ä–∏—à–ª–æ {arr.shape}")

    # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ Series —Å –∏–Ω–¥–µ–∫—Å–æ–º, –µ—Å–ª–∏ –æ–Ω –≤–∞–ª–∏–¥–µ–Ω
    if orig_index is None or len(orig_index) != len(arr):
        y1d = pd.Series(arr, dtype="float64")
    else:
        y1d = pd.Series(arr, index=orig_index, dtype="float64")

    df = y1d.to_frame(name="y")

    for lag in range(1, max_lag + 1):
        df[f"lag_{lag}"] = df["y"].shift(lag)
    for w in ma_windows:
        df[f"ma_{w}"] = df["y"].rolling(w, min_periods=1).mean().shift(1)

    # –î–æ–±–∞–≤–ª—è–µ–º –∏–Ω–¥–∏–∫–∞—Ç–æ—Ä—ã
    df = add_technical_indicators(df)

    df = df.dropna()
    return df

def ridge_fit_predict(y: pd.Series, test_len: int) -> EvalResult:
    """Ridge —Å –ø–æ–¥–±–æ—Ä–æ–º alpha —á–µ—Ä–µ–∑ GridSearch."""
    feat = build_lag_features(y, max_lag=10, ma_windows=[3, 7, 14])

    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ –ª–∞–≥–æ–≤ –º–∞–ª–æ —Å—Ç—Ä–æ–∫ ‚Äî —É–º–µ–Ω—å—à–∏–º —Ö–≤–æ—Å—Ç (–Ω–æ –Ω–µ –º–µ–Ω—å—à–µ 7)
    if len(feat) <= test_len:
        test_len = max(7, len(feat) // 5)

    X = feat.drop(columns=["y"])
    y_target = feat["y"]

    X_train, X_test = X.iloc[:-test_len], X.iloc[-test_len:]
    y_train, y_test = y_target.iloc[:-test_len], y_target.iloc[-test_len:]

    # –ü–æ–¥–±–æ—Ä alpha
    param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}
    grid = GridSearchCV(Ridge(random_state=42), param_grid, cv=3, scoring='neg_mean_squared_error')
    grid.fit(X_train, y_train)
    model = grid.best_estimator_

    preds = model.predict(X_test)

    return EvalResult(
        name="RIDGE",
        rmse=math.sqrt(mean_squared_error(y_test, preds)),
        mape=mape(y_test.values, preds),
        model_obj=(model, X.columns.tolist()),
        extra={"y_test_index": y_test.index, "preds": preds},
    )

def arima_fit_predict(y: pd.Series, test_len: int) -> EvalResult:
    """ARIMA —Å auto_arima –µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏–Ω–∞—á–µ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã."""
    train, test = train_test_split_series(y, test_len)
    
    if PMDARIMA_AVAILABLE:
        model = auto_arima(train, seasonal=False, trace=False, error_action='ignore', suppress_warnings=True)
        fit = model.fit(train)
        name = f"ARIMA{model.order}"
    else:
        order = (1, 1, 1)
        model_arima = ARIMA(train.values, order=order)
        fit = model_arima.fit()
        name = "ARIMA(1,1,1)"

    preds = fit.forecast(steps=len(test))
    preds = np.asarray(preds, dtype=float)

    res = EvalResult(
        name=name,
        rmse=math.sqrt(mean_squared_error(test.values, preds)),
        mape=mape(test.values, preds),
        model_obj=fit,
        extra={"y_test_index": test.index, "preds": preds},
    )
    return res

def lstm_fit_predict(y: pd.Series, test_len: int) -> Optional[EvalResult]:
    if not TF_AVAILABLE:
        return None

    series = y.values.astype("float32")
    train, test = train_test_split_series(y, test_len)
    tr = train.values.astype("float32")
    mn, mx = float(tr.min()), float(tr.max())
    rng = (mx - mn) if (mx - mn) > 1e-8 else 1.0

    norm = (series - mn) / rng
    window = 20

    X_all, y_all = make_lstm_dataset(norm, window)
    X_all = X_all.astype("float32")
    y_all = y_all.astype("float32")

    split_idx = len(train) - window
    if split_idx < 10:
        split_idx = int(max(10, 0.8 * len(X_all)))

    X_train, y_train = X_all[:split_idx], y_all[:split_idx]
    X_test, y_test = X_all[split_idx:], y_all[split_idx:]

    # –£–ª—É—á—à–µ–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞: dropout, early stopping
    model = keras.Sequential([
        layers.Input(shape=(window, 1)),
        layers.LSTM(64, return_sequences=True),
        layers.Dropout(0.2),
        layers.LSTM(32),
        layers.Dropout(0.2),
        layers.Dense(16, activation="relu"),
        layers.Dense(1),
    ])
    model.compile(optimizer="adam", loss="mse")
    
    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)
    model.fit(
        X_train,
        y_train,
        epochs=50,  # –£–≤–µ–ª–∏—á–µ–Ω–æ
        batch_size=16,
        verbose=0,
        validation_split=0.1,
        callbacks=[early_stop],
    )

    y_pred_norm = model.predict(X_test, verbose=0).ravel().astype("float32")
    y_test_denorm = y_test * rng + mn
    y_pred_denorm = y_pred_norm * rng + mn

    return EvalResult(
        name="LSTM",
        rmse=math.sqrt(mean_squared_error(y_test_denorm, y_pred_denorm)),
        mape=mape(y_test_denorm, y_pred_denorm),
        model_obj=(model, mn, mx, rng, window),
        extra={"y_test_index": y.index[-len(y_test_denorm):], "preds": y_pred_denorm},
    )

def pick_best_model(results: List[EvalResult]) -> EvalResult:
    """–í—ã–±–æ—Ä –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É —Ä–∞–Ω–≥—É RMSE/MAPE. –ú–µ–Ω—å—à–µ ‚Äî –ª—É—á—à–µ."""
    df = pd.DataFrame([
        {"name": r.name, "rmse": r.rmse, "mape": r.mape}
        for r in results
    ])
    df["rank_rmse"] = df["rmse"].rank(method="min")
    df["rank_mape"] = df["mape"].rank(method="min")
    df["rank_mean"] = (df["rank_rmse"] + df["rank_mape"]) / 2.0
    best_name = df.sort_values("rank_mean").iloc[0]["name"]
    for r in results:
        if r.name == best_name:
            return r
    return results[0]

def ensemble_forecast(results: List[EvalResult], y: pd.Series, horizon: int = 30) -> pd.Series:
    """–ê–Ω—Å–∞–º–±–ª—å: —É—Å—Ä–µ–¥–Ω—è–µ–º –ø—Ä–æ–≥–Ω–æ–∑—ã –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π —Å –≤–µ—Å–∞–º–∏ –ø–æ –æ–±—Ä–∞—Ç–Ω–æ–π MAPE."""
    forecasts = []
    weights = []
    last_date = y.index[-1]
    index = pd.date_range(last_date + pd.Timedelta(days=1), periods=horizon, freq="D")
    
    for res in results:
        fc = forecast_30(res, y)
        forecasts.append(fc.values)
        weights.append(1.0 / (res.mape + 1e-8))  # –í–µ—Å –ø–æ –æ–±—Ä–∞—Ç–Ω–æ–π MAPE
    
    weights = np.array(weights)
    weights /= weights.sum()  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è
    
    ensemble_preds = np.average(np.array(forecasts), axis=0, weights=weights)
    return pd.Series(ensemble_preds, index=index)

def forecast_30(best: EvalResult, y: pd.Series) -> pd.Series:
    """–î–µ–ª–∞–µ–º –ø—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 30 –¥–Ω–µ–π –≤–ø–µ—Ä—ë–¥ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–∏–ø–∞ –º–æ–¥–µ–ª–∏."""
    horizon = 30
    last_date = y.index[-1]

    if best.name == "RIDGE":
        model, cols = best.model_obj
        # –ò—Å—Ç–æ—Ä–∏—é –ø—Ä–∏–≤–æ–¥–∏–º –∫ 1D Series
        if isinstance(y, pd.DataFrame):
            y_num = y.select_dtypes(include=[np.number])
            base = y_num.iloc[:, 0]
        else:
            base = pd.Series(np.asarray(y).squeeze(), index=y.index, dtype="float64")

        hist = base.copy()
        preds = []

        for _ in range(horizon):
            feat_full = build_lag_features(hist, max_lag=10, ma_windows=[3, 7, 14])
            if feat_full.empty:
                # –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è –∏—Å—Ç–æ—Ä–∏—è ‚Äî –ø—Ä–µ—Ä—ã–≤–∞–µ–º—Å—è
                break
            feat = feat_full.iloc[-1:]
            x_row = feat.drop(columns=["y"]).copy()

            # –¥–æ–±–∏–≤–∞–µ–º –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω—É–ª—è–º–∏
            for c in cols:
                if c not in x_row.columns:
                    x_row[c] = 0.0
            x_row = x_row[cols]

            y_next = float(model.predict(x_row)[0])
            preds.append(y_next)

            # —Ä–∞—Å—à–∏—Ä—è–µ–º hist –∞–∫–∫—É—Ä–∞—Ç–Ω–æ
            next_idx = hist.index[-1] + pd.Timedelta(days=1)
            hist = pd.concat([hist, pd.Series([y_next], index=[next_idx], dtype="float64")])

        index = pd.date_range(last_date + pd.Timedelta(days=1), periods=len(preds), freq="D")
        return pd.Series(preds, index=index, dtype="float64")

    if best.name.startswith("ARIMA"):
        train, _ = train_test_split_series(y, 30)
        fit = best.model_obj
        fc = fit.forecast(steps=horizon)
        index = pd.date_range(last_date + pd.Timedelta(days=1), periods=horizon, freq="D")
        return pd.Series(np.asarray(fc, dtype=float), index=index)

    if best.name == "LSTM":
        model, mn, mx, rng, window = best.model_obj
        hist = y.values.astype("float32")
        preds = []

        norm_hist = (hist - mn) / (rng if rng > 1e-8 else 1.0)
        norm_hist = norm_hist.astype("float32")

        for _ in range(horizon):
            if len(norm_hist) < window:
                pad = np.zeros(window - len(norm_hist), dtype="float32")
                seq = np.concatenate([pad, norm_hist]).astype("float32")
            else:
                seq = norm_hist[-window:].astype("float32")

            x_np = np.asarray(seq, dtype="float32").reshape(1, window, 1)
            x = tf.convert_to_tensor(x_np, dtype=tf.float32)  # –∫–ª—é—á–µ–≤–∞—è —Å—Ç—Ä–æ–∫–∞
            y_next_norm = float(model.predict(x, verbose=0)[0][0])
            y_next = y_next_norm * (rng if rng > 1e-8 else 1.0) + mn

            preds.append(y_next)
            norm_hist = np.append(norm_hist, np.float32(y_next_norm)).astype("float32")

        index = pd.date_range(last_date + pd.Timedelta(days=1), periods=horizon, freq="D")
        return pd.Series(preds, index=index)

def local_extrema(series: pd.Series, window: int = 3) -> Tuple[List[pd.Timestamp], List[pd.Timestamp]]:
    """
    –ò—â–µ–º –ª–æ–∫–∞–ª—å–Ω—ã–µ –º–∏–Ω–∏–º—É–º—ã/–º–∞–∫—Å–∏–º—É–º—ã –Ω–∞ –ø—Ä–æ–≥–Ω–æ–∑–µ.
    –£—Å–ª–æ–≤–∏–µ –º–∏–Ω–∏–º—É–º–∞: –∑–Ω–∞—á–µ–Ω–∏–µ –º–µ–Ω—å—à–µ —Å–æ—Å–µ–¥–µ–π –≤ –æ–∫–Ω–µ; –º–∞–∫—Å–∏–º—É–º–∞ ‚Äî –±–æ–ª—å—à–µ —Å–æ—Å–µ–¥–µ–π.
    """
    lows, highs = [], []
    vals = series.values
    for i in range(window, len(vals) - window):
        seg = vals[i - window:i + window + 1]
        center = vals[i]
        if np.all(center <= seg) and np.count_nonzero(center < seg) >= 1:
            lows.append(series.index[i])
        if np.all(center >= seg) and np.count_nonzero(center > seg) >= 1:
            highs.append(series.index[i])
    return lows, highs

def simulate_strategy(history_last: float, forecast: pd.Series, cash: float) -> Tuple[float, List[Tuple[str, str, float, float]]]:
    """
    –ü—Ä–æ—Å—Ç–∞—è —Å–∏–º—É–ª—è—Ü–∏—è:
    - –ø–æ–∫—É–ø–∫–∞ –≤ –∫–∞–∂–¥–æ–º –ª–æ–∫–∞–ª—å–Ω–æ–º –º–∏–Ω–∏–º—É–º–µ –ø–æ —Ü–µ–Ω–µ –ø—Ä–æ–≥–Ω–æ–∑–∞,
    - –ø—Ä–æ–¥–∞–∂–∞ –≤ –±–ª–∏–∂–∞–π—à–µ–º –ª–æ–∫–∞–ª—å–Ω–æ–º –º–∞–∫—Å–∏–º—É–º–µ (–µ—Å–ª–∏ –æ–Ω –ø–æ–∑–∂–µ),
    - —Ç–æ—Ä–≥—É–µ–º –Ω–∞ –≤—Å—é —Å—É–º–º—É –∫–∞–∂–¥—ã–π —Ä–∞–∑, –±–µ–∑ –∫–æ–º–∏—Å—Å–∏–π.
    –í–æ–∑–≤—Ä–∞—Ç: –ø—Ä–∏–±—ã–ª—å –∏ —Å–¥–µ–ª–∫–∏ [(date_buy, date_sell, price_buy, price_sell), ...]
    """
    lows, highs = local_extrema(forecast, window=2)
    trades = []
    profit = 0.0
    # –°–æ–ø–æ—Å—Ç–∞–≤–∏–º –∫–∞–∂–¥–æ–π –ø–æ–∫—É–ø–∫–µ –±–ª–∏–∂–∞–π—à–∏–π –±—É–¥—É—â–∏–π –º–∞–∫—Å–∏–º—É–º
    for lb in lows:
        h_candidates = [h for h in highs if h > lb]
        if not h_candidates:
            continue
        hs = min(h_candidates)
        p_buy = float(forecast.loc[lb])
        p_sell = float(forecast.loc[hs])
        if p_sell <= p_buy:
            continue
        shares = cash / p_buy
        profit += (p_sell - p_buy) * shares
        trades.append((lb.strftime("%Y-%m-%d"), hs.strftime("%Y-%m-%d"), p_buy, p_sell))
    return profit, trades

def calculate_volatility(series: pd.Series) -> float:
    """–†–∞—Å—á—ë—Ç –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –∫–∞–∫ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–µ –æ—Ç–∫–ª–æ–Ω–µ–Ω–∏–µ –ø—Ä–æ—Ü–µ–Ω—Ç–Ω—ã—Ö –∏–∑–º–µ–Ω–µ–Ω–∏–π."""
    returns = series.pct_change().dropna()
    return returns.std() * 100  # –≤ –ø—Ä–æ—Ü–µ–Ω—Ç–∞—Ö

def calculate_trend(series: pd.Series) -> str:
    """–ü—Ä–æ—Å—Ç–æ–π —Ç—Ä–µ–Ω–¥: —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–≤–æ–π –∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ü–µ–Ω—ã."""
    if len(series) < 2:
        return "–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö"
    first = series.iloc[0]
    last = series.iloc[-1]
    pct = ((last - first) / first) * 100
    if pct > 1:
        return f"–í–æ—Å—Ö–æ–¥—è—â–∏–π (+{pct:.2f}%)"
    elif pct < -1:
        return f"–ù–∏—Å—Ö–æ–¥—è—â–∏–π ({pct:.2f}%)"
    else:
        return f"–ë–æ–∫–æ–≤–æ–π ({pct:.2f}%)"

def plot_history_forecast(hist: pd.Series, forecast: pd.Series, buys: List[pd.Timestamp], sells: List[pd.Timestamp], ticker: str, volatility: float, trend: str, delta_pct: float) -> bytes:
    """–°—Ç—Ä–æ–∏–º –≥—Ä–∞—Ñ–∏–∫ (–∏—Å—Ç–æ—Ä–∏—è + –ø—Ä–æ–≥–Ω–æ–∑) –∏ –ø–æ–º–µ—á–∞–µ–º —Ç–æ—á–∫–∏ –ø–æ–∫—É–ø–æ–∫/–ø—Ä–æ–¥–∞–∂. –î–æ–±–∞–≤–ª–µ–Ω—ã –ø–æ–¥–ø–∏—Å–∏, —Å—Ä–µ–¥–Ω—è—è –ª–∏–Ω–∏—è, –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ –∏–∑–º–µ–Ω–µ–Ω–∏–µ."""
    plt.figure(figsize=(12, 6))
    plt.plot(hist.index, hist.values, label="–ò—Å—Ç–æ—Ä–∏—è (Close)", color='blue', linewidth=1.5)
    plt.plot(forecast.index, forecast.values, label="–ü—Ä–æ–≥–Ω–æ–∑ (30 –¥.)", color='orange', linewidth=1.5, linestyle='--')
    
    # –°—Ä–µ–¥–Ω—è—è –ª–∏–Ω–∏—è –ø–æ –∏—Å—Ç–æ—Ä–∏–∏
    mean_hist = hist.mean()
    plt.axhline(y=mean_hist, color='green', linestyle=':', label=f'–°—Ä–µ–¥–Ω—è—è –∏—Å—Ç–æ—Ä–∏—è: {mean_hist:.2f}')
    
    # –û—Ç–º–µ—Ç–∏–º —ç–∫—Å—Ç—Ä–µ–º—É–º—ã
    buy_vals = [forecast.loc[d] for d in buys]
    sell_vals = [forecast.loc[d] for d in sells]
    plt.scatter(buys, buy_vals, marker="^", s=80, color='green', label="–ü–æ–∫—É–ø–∞—Ç—å")
    plt.scatter(sells, sell_vals, marker="v", s=80, color='red', label="–ü—Ä–æ–¥–∞–≤–∞—Ç—å")
    
    # –ü–æ–¥–ø–∏—Å–∏ –æ—Å–µ–π –∏ –∑–∞–≥–æ–ª–æ–≤–æ–∫
    plt.xlabel("–î–∞—Ç–∞")
    plt.ylabel("–¶–µ–Ω–∞ (USD)")
    plt.title(f"–ü—Ä–æ–≥–Ω–æ–∑ —Ü–µ–Ω—ã –∞–∫—Ü–∏–π {ticker}\n–ò–∑–º–µ–Ω–µ–Ω–∏–µ: {delta_pct:+.2f}%, –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {volatility:.2f}%, –¢—Ä–µ–Ω–¥: {trend}")
    plt.grid(True, alpha=0.3)
    plt.legend(loc='upper left')
    plt.xticks(rotation=45)
    plt.tight_layout()

    buf = io.BytesIO()
    plt.savefig(buf, format="png", dpi=150)
    plt.close()
    buf.seek(0)
    return buf.read()

def parse_user_text(text: str) -> Tuple[str, float]:
    """
    –û–∂–∏–¥–∞–µ–º—ã–π —Ñ–æ—Ä–º–∞—Ç: 'GOOGL 20000' –∏–ª–∏ 'MSFT, 15000'
    –í–æ–∑–≤—Ä–∞—â–∞–µ–º (—Ç–∏–∫–µ—Ä UPPER, —Å—É–º–º–∞).
    """
    raw = text.replace(",", " ").replace(";", " ").strip().split()
    if len(raw) < 2:
        raise ValueError("–£–∫–∞–∂–∏—Ç–µ —Ç–∏–∫–µ—Ä –∏ —Å—É–º–º—É –≤ USD, –Ω–∞–ø—Ä–∏–º–µ—Ä: GOOGL 20000")
    ticker = raw[0].upper()
    try:
        amount = float(raw[1])
    except Exception:
        raise ValueError("–°—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —á–∏—Å–ª–æ–º. –ü—Ä–∏–º–µ—Ä: GOOGL 20000")
    if amount <= 0:
        raise ValueError("–°—É–º–º–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –±–æ–ª—å—à–µ –Ω—É–ª—è.")
    return ticker, amount

def safe_log(row: Dict[str, object], path: str = "logs.csv") -> None:
    """–ó–∞–ø–∏—Å—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫—É –≤ CSV (—Å–æ–∑–¥–∞—ë–º —Ñ–∞–π–ª —Å –∑–∞–≥–æ–ª–æ–≤–∫–æ–º –ø—Ä–∏ –ø–µ—Ä–≤–æ–º –∑–∞–ø—É—Å–∫–µ)."""
    cols = [
        "timestamp", "user_id", "ticker", "amount",
        "best_model", "rmse", "mape",
        "delta_pct", "profit_est"
    ]
    exists = os.path.exists(path)
    df = pd.DataFrame([row], columns=cols)
    if exists:
        df.to_csv(path, index=False, mode="a", header=False, encoding="utf-8")
    else:
        df.to_csv(path, index=False, mode="w", header=True, encoding="utf-8")

def load_dotenv(path: str = ".env"):
    if not os.path.exists(path):
        return
    for line in open(path, "r", encoding="utf-8"):
        if "=" in line and not line.strip().startswith("#"):
            key, val = line.strip().split("=", 1)
            os.environ[key] = val
load_dotenv()


# –¢–ï–õ–ï–ì–†–ê–ú-–ë–û–¢ (Aiogram)

TOKEN = os.getenv("TOKEN")

def make_kb() -> types.InlineKeyboardMarkup:
    kb = InlineKeyboardBuilder()
    kb.button(text="GOOGL 20000", callback_data="GOOGL 20000")
    kb.button(text="MSFT 15000", callback_data="MSFT 15000")
    kb.button(text="GOOGL 20000", callback_data="GOOGL 20000")
    kb.button(text="TSLA 5000", callback_data="TSLA 5000")
    kb.button(text="AMZN 25000", callback_data="AMZN 25000")
    kb.button(text="–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –≤—Å–µ—Ö —Ç–∏–∫–µ—Ä–æ–≤", callback_data="show_tickers")
    kb.adjust(2)  # 2 –∫–Ω–æ–ø–∫–∏ –≤ —Ä—è–¥
    return kb.as_markup()

def make_tickers_kb() -> types.InlineKeyboardMarkup:
    kb = InlineKeyboardBuilder()
    for t in TOP_TICKERS:
        kb.button(text=t, callback_data=f"ticker_{t}")
    kb.button(text="–ù–∞–∑–∞–¥", callback_data="back")
    kb.adjust(3)  # 3 –∫–Ω–æ–ø–∫–∏ –≤ —Ä—è–¥
    return kb.as_markup()

async def process_query(text: str, user: types.User) -> None:
    """–û—Å–Ω–æ–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫: –ø–∞—Ä—Å–∏–Ω–≥, –∑–∞–≥—Ä—É–∑–∫–∞, –æ–±—É—á–µ–Ω–∏–µ, –æ—Ç–≤–µ—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é."""
    try:
        ticker, amount = parse_user_text(text)
    except Exception as e:
        await user.bot.send_message(chat_id=user.id, text=f"‚õî {e}", reply_markup=make_kb())
        return

    await user.bot.send_message(chat_id=user.id, text=f"–ó–∞–ø—Ä–æ—Å –ø—Ä–∏–Ω—è—Ç: {ticker}, —Å—É–º–º–∞ {amount:.2f}.\n–ì–æ—Ç–æ–≤–ª—é –¥–∞–Ω–Ω—ã–µ‚Ä¶")

    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ—Ç–∏—Ä–æ–≤–æ–∫
    end = datetime.now(timezone.utc).date()
    start = end - timedelta(days=365*2)  # 2 –≥–æ–¥–∞
    logger.info(f"–ù–∞—á–∏–Ω–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}: start={start}, end={end}")

    data = None
    try:
        # –ü–æ–ø—ã—Ç–∫–∞ 1: yfinance
        data = yf.download(
            ticker,
            start=start,
            end=end + timedelta(days=1),
            interval="1d",
            progress=False,
            auto_adjust=False,
            timeout=10
        )
        logger.info(f"yfinance: –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è {ticker}: shape={data.shape}")
        if data.empty:
            raise ValueError("yfinance –≤–µ—Ä–Ω—É–ª –ø—É—Å—Ç—ã–µ –¥–∞–Ω–Ω—ã–µ")
    except Exception as e:
        logger.warning(f"yfinance failed –¥–ª—è {ticker}: {e}. –ü—Ä–æ–±—É–µ–º pandas_datareader...")
        try:
            # –ü–æ–ø—ã—Ç–∫–∞ 2: pandas_datareader (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞)
            import pandas_datareader as pdr
            data = pdr.get_data_yahoo(ticker, start=start, end=end)
            logger.info(f"pandas_datareader: –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –¥–ª—è {ticker}: shape={data.shape}")
        except Exception as e2:
            logger.exception(f"pandas_datareader failed –¥–ª—è {ticker}: {e2}")
            await user.bot.send_message(chat_id=user.id, text="–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ—Ç–∏—Ä–æ–≤–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–∫–µ—Ä —Å–≤–µ—Ä–∏–≤ –µ–≥–æ —Å –≤–ª–æ–∂–µ–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
            return

    if data.empty or "Close" not in data.columns:
        logger.warning(f"–î–∞–Ω–Ω—ã–µ –ø—É—Å—Ç—ã–µ –∏–ª–∏ –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'Close' –¥–ª—è {ticker}: empty={data.empty}, columns={list(data.columns)}")
        await user.bot.send_message(chat_id=user.id, text="–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –∫–æ—Ç–∏—Ä–æ–≤–∫–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —Ç–∏–∫–µ—Ä —Å–≤–µ—Ä–∏–≤ –µ–≥–æ —Å –≤–ª–æ–∂–µ–Ω—ã–º —Å–ø–∏—Å–∫–æ–º –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")
        return

    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π)
    close_obj = data["Close"] if "Close" in data.columns else data.get("Close")
    if isinstance(close_obj, pd.DataFrame):
        num_cols = close_obj.select_dtypes(include=[np.number])
        if num_cols.shape[1] == 0:
            logger.warning(f"–í –∫–æ–ª–æ–Ω–∫–µ Close –Ω–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {ticker}")
            await user.bot.send_message(chat_id=user.id, text="–í –∫–æ–ª–æ–Ω–∫–µ Close –Ω–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ç–∏–∫–µ—Ä –∏–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –∏–∑ —Å–ø–∏—Å–∫–∞.")
            return
        close = num_cols.iloc[:, 0].copy()
    else:
        close = pd.Series(close_obj, copy=True)

    close = close.dropna()
    close.index = pd.to_datetime(close.index)
    close = pd.Series(np.asarray(close).squeeze(), index=close.index, dtype="float64")
    
    # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
    close = remove_outliers(close)
    
    logger.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {ticker}: len={len(close)}, last_date={close.index[-1] if not close.empty else 'N/A'}")

    if len(close) < 80:
        logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è {ticker}: len={len(close)} < 80")
        await user.bot.send_message(chat_id=user.id, text="–ò—Å—Ç–æ—Ä–∏–∏ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è. –ù—É–∂–µ–Ω —Ö–æ—Ç—è –±—ã –∫–≤–∞—Ä—Ç–∞–ª –ø–ª–æ—Ç–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö.")
        return

    test_days = 30
    results: List[EvalResult] = []

    try:
        logger.info(f"–û–±—É—á–µ–Ω–∏–µ RIDGE –¥–ª—è {ticker}")
        results.append(ridge_fit_predict(close, test_days))
        logger.info(f"RIDGE –æ–±—É—á–µ–Ω –¥–ª—è {ticker}")
    except Exception as e:
        logger.exception(f"RIDGE error –¥–ª—è {ticker}: {e}")

    try:
        logger.info(f"–û–±—É—á–µ–Ω–∏–µ ARIMA –¥–ª—è {ticker}")
        results.append(arima_fit_predict(close, test_days))
        logger.info(f"ARIMA –æ–±—É—á–µ–Ω –¥–ª—è {ticker}")
    except Exception as e:
        logger.exception(f"ARIMA error –¥–ª—è {ticker}: {e}")

    if TF_AVAILABLE:
        try:
            logger.info(f"–û–±—É—á–µ–Ω–∏–µ LSTM –¥–ª—è {ticker}")
            lstm_res = lstm_fit_predict(close, test_days)
            if lstm_res is not None:
                results.append(lstm_res)
                logger.info(f"LSTM –æ–±—É—á–µ–Ω –¥–ª—è {ticker}")
            else:
                logger.warning(f"LSTM –≤–µ—Ä–Ω—É–ª None –¥–ª—è {ticker}")
        except Exception as e:
            logger.exception(f"LSTM error –¥–ª—è {ticker}: {e}")

    if not results:
        logger.error(f"–í—Å–µ –º–æ–¥–µ–ª–∏ —É–ø–∞–ª–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏ –¥–ª—è {ticker}")
        await user.bot.send_message(chat_id=user.id, text="–í—Å–µ –º–æ–¥–µ–ª–∏ —É–ø–∞–ª–∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –¥—Ä—É–≥–æ–π —Ç–∏–∫–µ—Ä.")
        return

    best = pick_best_model(results)

    # –ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 30 –¥–Ω–µ–π ‚Äî –∞–Ω—Å–∞–º–±–ª—å –¥–ª—è –ª—É—á—à–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
    forecast = ensemble_forecast(results, close, 30)

    # –ò–∑–º–µ–Ω–µ–Ω–∏–µ –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π —Ü–µ–Ω—ã
    last_price = float(close.iloc[-1])
    last_forecast = float(forecast.iloc[-1])
    delta_pct = ((last_forecast - last_price) / last_price) * 100.0

    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    volatility = calculate_volatility(close)
    trend = calculate_trend(close)

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: –ª–æ–∫–∞–ª—å–Ω—ã–µ —ç–∫—Å—Ç—Ä–µ–º—É–º—ã –ø—Ä–æ–≥–Ω–æ–∑–∞
    lows, highs = local_extrema(forecast, window=2)
    profit_est, trades = simulate_strategy(last_price, forecast, amount)

    # –ì—Ä–∞—Ñ–∏–∫
    img_bytes = plot_history_forecast(close[-120:], forecast, lows, highs, ticker, volatility, trend, delta_pct)  # –∏—Å—Ç–æ—Ä–∏—è —Ç–æ–ª—å–∫–æ –∑–∞ ~–ø–æ–ª–≥–æ–¥–∞ –¥–ª—è —á–∏—Ç–∞–±–µ–ª—å–Ω–æ—Å—Ç–∏

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç
    lines = []
    lines.append(f"–õ—É—á—à–∏–π –∞–ª–≥–æ—Ä–∏—Ç–º: *{best.name}*")
    lines.append(f"RMSE: `{best.rmse:.4f}`, MAPE: `{best.mape:.2f}%`")
    lines.append(f"–¢–µ–∫—É—â–∞—è —Ü–µ–Ω–∞: *{last_price:.2f} USD*")
    lines.append(f"–í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å (–≥–æ–¥): `{volatility:.2f}%`")
    lines.append(f"–¢—Ä–µ–Ω–¥ (–≥–æ–¥): *{trend}*")
    lines.append(f"–ü—Ä–æ–≥–Ω–æ–∑ –Ω–∞ 30 –¥–Ω–µ–π: –∏–∑–º–µ–Ω–µ–Ω–∏–µ –∫ –ø–æ—Å–ª–µ–¥–Ω–µ–º—É –¥–Ω—é: *{delta_pct:+.2f}%*")
    if trades:
        lines.append(f"–ù–∞–π–¥–µ–Ω–æ —Å–¥–µ–ª–æ–∫ –ø–æ —Å–∏–≥–Ω–∞–ª–∞–º: *{len(trades)}*")
        preview = "\n".join([f"‚Ä¢ {b} ‚Üí {s}: {pb:.2f} ‚Üí {ps:.2f}" for (b, s, pb, ps) in trades[:5]])
        lines.append(preview)
    else:
        lines.append("–°–∏–≥–Ω–∞–ª—ã –¥–ª—è –ø–æ–∫—É–ø–æ–∫/–ø—Ä–æ–¥–∞–∂ –ø–æ –ª–æ–∫–∞–ª—å–Ω—ã–º —ç–∫—Å—Ç—Ä–µ–º—É–º–∞–º –Ω–µ –≤—ã—Ä–∞–∂–µ–Ω—ã.")
    lines.append(f"–û—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–æ—á–Ω–∞—è –ø—Ä–∏–±—ã–ª—å –æ—Ç —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –Ω–∞ —Å—É–º–º—É {amount:.2f}: *{profit_est:.2f}* —É.–µ.")
    lines.append(f"*‚ö†Ô∏è –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–æ—Å—è—Ç —É—á–µ–±–Ω—ã–π —Ö–∞—Ä–∞–∫—Ç–µ—Ä –∏ –Ω–µ —è–≤–ª—è—é—Ç—Å—è –∏–Ω–≤–µ—Å—Ç—Å–æ–≤–µ—Ç–æ–º.*")
    text = "\n".join(lines)

    # –û—Ç–ø—Ä–∞–≤–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    await user.bot.send_photo(
        chat_id=user.id,
        photo=types.BufferedInputFile(img_bytes, filename=f"{ticker}_forecast.png"),
        caption=text,
        parse_mode=ParseMode.MARKDOWN,
    )

    # –õ–æ–≥
    row = {
        "timestamp": datetime.now(timezone.utc).isoformat(timespec="seconds"),
        "user_id": str(user.id),
        "ticker": ticker,
        "amount": amount,
        "best_model": best.name,
        "rmse": round(best.rmse, 6),
        "mape": round(best.mape, 4),
        "delta_pct": round(delta_pct, 4),
        "profit_est": round(profit_est, 4),
    }
    try:
        safe_log(row, path="logs.csv")
    except Exception as e:
        logger.warning("log write failed: %s", e)

async def handle_query(message: types.Message) -> None:
    await process_query(message.text, message.from_user)

def main() -> None:
    token = TOKEN
    if not token:
        raise RuntimeError("–ù–µ –Ω–∞–π–¥–µ–Ω TOKEN –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è. –°–æ–∑–¥–∞–π—Ç–µ .env –∏–ª–∏ —ç–∫—Å–ø–æ—Ä—Ç–∏—Ä—É–π—Ç–µ —Ç–æ–∫–µ–Ω.")
    bot = Bot(token=token)
    dp = Dispatcher()

    @dp.message(Command("start"))
    async def start_cmd(message: types.Message) -> None:
        user_name = message.from_user.first_name or "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å"
        await message.answer(
            f"üôÇ –ü—Ä–∏–≤–µ—Ç, {user_name}! –ü—Ä–∏—à–ª–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ —Ç–∏–∫–µ—Ä –∏ —Å—É–º–º—É. –ù–∞–ø—Ä–∏–º–µ—Ä: `GOOGL 20000`\n–ò–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ –º–µ–Ω—é:",
            parse_mode=ParseMode.MARKDOWN,
            reply_markup=make_kb(),
        )

    @dp.message(F.text)
    async def any_text(message: types.Message) -> None:
        user_id = message.from_user.id
        if user_id in user_states and user_states[user_id]["state"] == "waiting_for_amount":
            ticker = user_states[user_id]["ticker"]
            try:
                amount = float(message.text.strip())
                if amount <= 0:
                    raise ValueError
                del user_states[user_id]
                await process_query(f"{ticker} {amount}", message.from_user)
            except ValueError:
                await message.answer("–í–≤–µ–¥–∏—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω—É—é —Å—É–º–º—É (—á–∏—Å–ª–æ > 0):")
        else:
            await handle_query(message)

    @dp.callback_query(F.data)
    async def process_callback(callback: types.CallbackQuery) -> None:
        await callback.answer()
        data = callback.data
        if data == "show_tickers":
            await callback.message.edit_text("–í—ã–±–µ—Ä–∏—Ç–µ —Ç–∏–∫–µ—Ä:", reply_markup=make_tickers_kb())
        elif data == "back":
            await callback.message.edit_text(
                "–ü—Ä–∏—à–ª–∏—Ç–µ –ø–æ–∂–∞–ª—É–π—Å—Ç–∞ —Ç–∏–∫–µ—Ä –∏ —Å—É–º–º—É. –ù–∞–ø—Ä–∏–º–µ—Ä: `GOOGL 20000`\n–ò–ª–∏ –≤—ã–±–µ—Ä–∏—Ç–µ –∏–∑ —Å–ø–∏—Å–∫–∞ –≤ –º–µ–Ω—é:",
                reply_markup=make_kb(),
            )
        elif data.startswith("ticker_"):
            ticker = data.split("_", 1)[1]
            user_id = callback.from_user.id
            user_states[user_id] = {"state": "waiting_for_amount", "ticker": ticker}
            await callback.message.edit_text(f"–í—ã–±–µ—Ä–∏—Ç–µ —Å—É–º–º—É –¥–ª—è {ticker} –≤ USD (–Ω–∞–ø—Ä–∏–º–µ—Ä, 10000):", reply_markup=None)
        else:
            await process_query(data, callback.from_user)

    dp.run_polling(bot)

if __name__ == "__main__":
    main()
